# -*- coding: utf-8 -*-
"""Mini_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FlvyRdy6fHBZe8JgIe_kfT3K0dcGeumh

# Rohit Jagtap (202201040048)
# Om Pawar (202201040010)
# Parth Kulkarni (202201040007)

## **MINI Project**

## **Importing Libraries**
"""

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import StackingClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
df1 = pd.read_csv('bitcoin_test_dataset.csv')
df2 = pd.read_csv('bitcoin_train_dataset.csv')

"""#### Combining both datasets into one dataframe"""

df = pd.concat([df1,df2],ignore_index= True)

"""#### Deleting the duplicated rows"""

df.drop_duplicates() # there is more 1000 duplicated row

"""## **Displaying Basic info**"""

# displaying first 5 rows
df.head()

# Understanding data types and number of non-null rows
df.info()

"""## **Handling Missing Values**

**1. Calculating number of missing values in each column**
"""

df.isna().sum()

df.isna().sum() / len(df) * 100 # Missing Values Percentage %

"""**2. Checking the Frequency Distribution of Columns with Null Values**"""

# Columns to plot
columns = ['Open', 'High', 'Close']
colors = ['skyblue', 'salmon', 'green']
titles = ['Distribution of Open Prices', 'Distribution of High Prices', 'Distribution of Close Prices']

# Set up the plot size
plt.figure(figsize=(15, 5))

# Plot histograms for the columns
for i, col in enumerate(columns):
    plt.subplot(1, 3, i + 1)
    sns.histplot(df[col], kde=True, bins=20, color=colors[i])
    plt.title(titles[i])

plt.tight_layout()
plt.show()

"""**3. Checking the boxplots of each columns and their central values**"""

# Volume box plot
plt.figure(figsize=(14, 5))
sns.boxplot(x=df['Volume'])
plt.title('Box Plot of Volume')
plt.show()

# Printing central values
print("Median:", df['Volume'].median(), "\n",
      "Mean:", df['Volume'].mean(), "\n")

# Create a categorical column for demonstration
df['Adj Close Category'] = pd.cut(df['Adj Close'], bins=2, labels=["Low", "High"])

# Swarm plot
plt.figure(figsize=(18, 7))
sns.swarmplot(data=df, x='Open', hue='Adj Close Category')
plt.title('Swarm Plot of Open Prices by Adjusted Close Category')
plt.show()

# Printing central values
print("Median:", df['Open'].median(), "\n",
      "Mean:", df['Open'].mean(), "\n")

# Box plot for 'High'
plt.figure(figsize=(14, 5))
sns.boxplot(x=df['High'])
plt.title('Box Plot of High Prices')
plt.show()

# Printing central values
print("Median:", df['High'].median(), "\n",
      "Mean:", df['High'].mean(), "\n")

"""**4. Filling Missing values**"""

# Fill missing values in the numeric columns
df['Open'] = df['Open'].fillna(df['Open'].mean())
df['High'] = df['High'].fillna(df['High'].median())
df['Volume'] = df['Volume'].fillna(df['Volume'].mean())

"""**5. Confirming All missing values are cleaned**"""

df.isna().sum()

"""## **Understanding Potability Values**
- To check if there is correlation between colums
"""

# Create a categorical column for demonstration
df['Price Category'] = pd.cut(df['Adj Close'], bins=3, labels=['Low', 'Medium', 'High'])

# Counting the categories
plt.figure(figsize=(14, 5))
sns.countplot(x=df['Price Category'])
plt.title('Count of Price Categories')
plt.show()

# Create a categorical column for demonstration
df['Adj Close Category'] = pd.cut(df['Adj Close'], bins=2, labels=["Low", "High"])

# Box plot for Open Prices by Adjusted Close Category
plt.figure(figsize=(18, 7))
sns.boxplot(data=df, y='Open', x='Adj Close Category')
plt.title('Box Plot of Open Prices by Adjusted Close Category')
plt.show()

# Create a categorical column for demonstration
df['Price Category'] = pd.cut(df['Adj Close'], bins=3, labels=["Low", "Medium", "High"])

# Box plot for Volume by Price Category
plt.figure(figsize=(16, 7))
sns.boxplot(data=df, y='Volume', x='Price Category')
plt.title('Box Plot of Volume by Price Category')
plt.show()

# Create a categorical column for demonstration
df['Price Category'] = pd.cut(df['Adj Close'], bins=3, labels=["Low", "Medium", "High"])

# Box plot for High Prices by Price Category
plt.figure(figsize=(15, 7))
sns.boxplot(data=df, y='High', x='Price Category')
plt.title('Box Plot of High Prices by Price Category')
plt.show()

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Compute the correlation matrix for numeric columns
df_corr = numeric_df.corr() * 100

# Print correlations with 'Adj Close' (or another numeric column of interest)
df_corr['Adj Close'].sort_values(ascending=False)

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Compute the correlation matrix for numeric columns
plt.figure(figsize=(12, 5))
sns.heatmap(numeric_df.corr(), cmap='coolwarm', annot=True, fmt='.2f')
plt.title("Correlation of Numeric Features")
plt.show()

"""## **Data Visualizations**

**1.Frequency Distribution of each feature**
"""

plt.figure(figsize=(15, 10))

# List of features to visualize, excluding the target or unnecessary columns if any
# Replace 'TargetColumnName' with the actual target column (if any, e.g., 'Close' for prediction)
features = df.columns.drop('Close') if 'Close' in df.columns else df.columns

# Calculate the number of rows and columns for subplots
n_features = len(features)
n_cols = 3  # Number of columns in the grid
n_rows = (n_features + n_cols - 1) // n_cols  # Calculate rows needed

# Create a subplot for each feature
for i, feature in enumerate(features):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.histplot(df[feature].dropna(), kde=True, color='skyblue')  # Handle missing data
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

# Adjust layout to avoid overlapping
plt.tight_layout()
plt.show()

"""**2.Pairplot**"""

# Create a categorical column for demonstration
df['Price Category'] = pd.cut(df['Adj Close'], bins=3, labels=["Low", "Medium", "High"])

# Set up the plot size
plt.figure(figsize=(10, 8))

# Use seaborn pairplot to visualize relationships between features, using 'Price Category' as hue
sns.pairplot(df, hue='Price Category', palette='Set2', markers=["o", "s"])

# Set the title
plt.suptitle('Pairplot of Features with Price Category', y=1.02)

# Show the plot
plt.tight_layout()
plt.show()

"""**3.Feature Importance**"""

# Drop non-numeric columns (e.g., 'Date')
df_numeric = df.select_dtypes(include=['float64', 'int64'])

# Assuming 'Adj Close' is the target variable for regression
X = df_numeric.drop('Adj Close', axis=1)  # Features
y = df_numeric['Adj Close']  # Target variable

# Initialize the Random Forest model for regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf.fit(X, y)

# Get the feature importance scores
feature_importances = rf.feature_importances_

# Create a DataFrame for better visualization
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# Sort the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Set up the plot
plt.figure(figsize=(10, 6))

# Plot the feature importances as a bar plot
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)

# Set the title and labels
plt.title('Feature Importance (Random Forest)', fontsize=16)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)

# Show the plot
plt.tight_layout()
plt.show()

# Print the feature importance scores
print(feature_importance_df)

# Drop non-numeric columns (e.g., 'Date')
df_numeric = df.select_dtypes(include=['float64', 'int64'])

# Create a binary target column (for example, based on 'Adj Close')
df_numeric['Price Category'] = pd.cut(df_numeric['Adj Close'], bins=2, labels=[0, 1])  # 0 for low, 1 for high

# Split the data into features (X) and target (y)
X = df_numeric.drop(['Adj Close', 'Price Category'], axis=1)  # Features
y = df_numeric['Price Category']  # Target variable (binary classification)

# Apply BorderlineSMOTE for balancing the classes
smote = BorderlineSMOTE(random_state=42, kind='borderline-2')
X_smote, y_smote = smote.fit_resample(X, y)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)

# Check the shape of the new data after oversampling
print(f"Original target distribution: {y.value_counts()}")
print(f"Resampled target distribution: {y_smote.value_counts()}")

# Plot the class distribution after applying SMOTE
y_smote.value_counts().plot(kind='bar', color=["#bcbddc", "#FF5733"])

# Title and labels
plt.title("Countplot for Price Category (After SMOTE)", fontsize=14)
plt.xlabel("Price Category", fontsize=12)
plt.ylabel("Count", fontsize=12)

# Display the plot
plt.show()

"""### **Model 1.1: Random Forest**"""

# Define the RandomForestClassifier with the best hyperparameters
rf = RandomForestClassifier(bootstrap=True,max_depth=None,min_samples_leaf=2,min_samples_split=2,n_estimators=200
)

rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

# Calculate metrics
rf_accuracy = round(accuracy_score(y_test, y_pred), 3)
rf_precision = round(precision_score(y_test, y_pred, average='weighted'), 3)
rf_recall = round(recall_score(y_test, y_pred, average='weighted'), 3)
rf_f1 = round(f1_score(y_test, y_pred, average='weighted'), 3)

# Printing classification results
print("Random Forest Classification Results with Best Hyperparameters:")
print(f"Accuracy: {rf_accuracy}")
print(f"Precision: {rf_precision}")
print(f"Recall: {rf_recall}")
print(f"F1-Score: {rf_f1}")

# Storing results in a dict
rf_results = {
    'Model': 'Random Forest with Best Hyperparameters',
    'Accuracy': rf_accuracy,
    'Precision': rf_precision,
    'Recall': rf_recall,
    'F1-Score': rf_f1,
    'Best Hyperparameters': {
        'bootstrap': True,
        'max_depth': None,
        'min_samples_leaf': 2,
        'min_samples_split': 2,
        'n_estimators': 200
    }
}

"""### **Model 1.2: Gaussian Naive Bayes**"""

# Initialize the Gaussian Naive Bayes model with the best hyperparameters found from RandomizedSearchCV
gnb = GaussianNB(var_smoothing=2.310129700083158e-06)
gnb.fit(X_train, y_train)
y_pred_gnb = gnb.predict(X_test)

# Calculate metrics with zero_division handling
gnb_accuracy = round(accuracy_score(y_test, y_pred_gnb), 3)
gnb_precision = round(precision_score(y_test, y_pred_gnb, average='weighted', zero_division=1), 3)
gnb_recall = round(recall_score(y_test, y_pred_gnb, average='weighted'), 3)
gnb_f1 = round(f1_score(y_test, y_pred_gnb, average='weighted'), 3)

# Print classification results
print("Gaussian Naive Bayes Classification Results with Best Hyperparameters:")
print(f"Accuracy: {gnb_accuracy}")
print(f"Precision: {gnb_precision}")
print(f"Recall: {gnb_recall}")
print(f"F1-Score: {gnb_f1}")

# Store results in a dictionary for later comparison
gnb_results = {
    'Model': 'Gaussian Naive Bayes with Best Hyperparameters',
    'Accuracy': gnb_accuracy,
    'Precision': gnb_precision,
    'Recall': gnb_recall,
    'F1-Score': gnb_f1,
    'Best Hyperparameters': {'var_smoothing': 2.310129700083158e-06}
}

"""### **Model 1.3: XGBoosting Classifier**"""

# Define the XGBoost model with the best hyperparameters
xgb_model = xgb.XGBClassifier(subsample=0.8,reg_lambda=0.2,reg_alpha=0.1,n_estimators=300,max_depth=7,learning_rate=0.1,gamma=0.4,colsample_bytree=1.0
)

# Fit the model to the training data
xgb_model.fit(X_train, y_train)

# Make predictions with the model
y_pred_xgb = xgb_model.predict(X_test)

# Calculate metrics
xgb_accuracy = round(accuracy_score(y_test, y_pred_xgb), 3)
xgb_precision = round(precision_score(y_test, y_pred_xgb, average='weighted', zero_division=1), 3)
xgb_recall = round(recall_score(y_test, y_pred_xgb, average='weighted'), 3)
xgb_f1 = round(f1_score(y_test, y_pred_xgb, average='weighted'), 3)

# Print classification results
print("XGBoost Classification Results with Best Hyperparameters:")
print(f"Accuracy: {xgb_accuracy}")
print(f"Precision: {xgb_precision}")
print(f"Recall: {xgb_recall}")
print(f"F1-Score: {xgb_f1}")

# Store results in a dictionary for later comparison
xgb_results = {
    'Model': 'XGBoost with Best Hyperparameters',
    'Accuracy': xgb_accuracy,
    'Precision': xgb_precision,
    'Recall': xgb_recall,
    'F1-Score': xgb_f1
}

"""### **Model 1.4: Adaboost Classifier**"""

# Initialize the AdaBoost classifier with the best hyperparameters
ada_boost = AdaBoostClassifier(learning_rate=1.0, n_estimators=150,algorithm='SAMME')

# Train the model
ada_boost.fit(X_train, y_train)

# Make predictions
y_pred_ada = ada_boost.predict(X_test)

# Calculate metrics
ada_accuracy = round(accuracy_score(y_test, y_pred_ada), 3)
ada_precision = round(precision_score(y_test, y_pred_ada, average='weighted', zero_division=1), 3)
ada_recall = round(recall_score(y_test, y_pred_ada, average='weighted'), 3)
ada_f1 = round(f1_score(y_test, y_pred_ada, average='weighted'), 3)

# Print classification results
print("AdaBoost Classification Results:")
print(f"Accuracy: {ada_accuracy}")
print(f"Precision: {ada_precision}")
print(f"Recall: {ada_recall}")
print(f"F1-Score: {ada_f1}")

# Store results in a dictionary for later comparison
ada_results = {
    'Model': 'AdaBoost',
    'Accuracy': ada_accuracy,
    'Precision': ada_precision,
    'Recall': ada_recall,
    'F1-Score': ada_f1
}

"""# **TRAINING 2 (Scale Sensitive Models)**

### **Scaling**
"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""### **Model 2.1: Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Train the Logistic Regression model with the best hyperparameters
lr = LogisticRegression(C=0.1, max_iter=500, penalty='l2', solver='lbfgs')  # Using the best hyperparameters
lr.fit(X_train_scaled, y_train)

# Make predictions
y_pred_lr = lr.predict(X_test_scaled)

# Calculate metrics
lr_accuracy = round(accuracy_score(y_test, y_pred_lr), 3)
lr_precision = round(precision_score(y_test, y_pred_lr, average='weighted', zero_division=1), 3)
lr_recall = round(recall_score(y_test, y_pred_lr, average='weighted'), 3)
lr_f1 = round(f1_score(y_test, y_pred_lr, average='weighted'), 3)

# Print classification results
print("Logistic Regression Classification Results with Best Hyperparameters:")
print(f"Accuracy: {lr_accuracy}")
print(f"Precision: {lr_precision}")
print(f"Recall: {lr_recall}")
print(f"F1-Score: {lr_f1}")

# Store results in a dictionary for later comparison
lr_results = {
    'Model': 'Logistic Regression',
    'Accuracy': lr_accuracy,
    'Precision': lr_precision,
    'Recall': lr_recall,
    'F1-Score': lr_f1
}

"""### **Model 2.2: SVM**"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Instantiate the SVM model with the best hyperparameters
svm = SVC(C=0.1, kernel='rbf', gamma='scale', degree=3, coef0=0.0)  # Use the hyperparameters you obtained

# Fit the model to the scaled training data
svm.fit(X_train_scaled, y_train)

# Make predictions using the trained model
y_pred_svm = svm.predict(X_test_scaled)

# Calculate the classification metrics
svm_accuracy = round(accuracy_score(y_test, y_pred_svm), 3)
svm_precision = round(precision_score(y_test, y_pred_svm, average='weighted', zero_division=1), 3)
svm_recall = round(recall_score(y_test, y_pred_svm, average='weighted'), 3)
svm_f1 = round(f1_score(y_test, y_pred_svm, average='weighted'), 3)

# Print the results
print("SVM Classification Results with Predefined Hyperparameters:")
print(f"Accuracy: {svm_accuracy}")
print(f"Precision: {svm_precision}")
print(f"Recall: {svm_recall}")
print(f"F1-Score: {svm_f1}")

# Store results in a dictionary for later comparison
svm_results = {
    'Model': 'SVM with Predefined Hyperparameters',
    'Accuracy': svm_accuracy,
    'Precision': svm_precision,
    'Recall': svm_recall,
    'F1-Score': svm_f1
}

"""### **Model 2.3: KNN**"""

# Train the KNN classifier and make predictions
knn = KNeighborsClassifier(n_neighbors=5)  # Default is 5 neighbors; can be tuned
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# Calculate metrics with zero_division handling
knn_accuracy = round(accuracy_score(y_test, y_pred_knn), 3)
knn_precision = round(precision_score(y_test, y_pred_knn, average='weighted', zero_division=1), 3)
knn_recall = round(recall_score(y_test, y_pred_knn, average='weighted'), 3)
knn_f1 = round(f1_score(y_test, y_pred_knn, average='weighted'), 3)

# Print classification results
print("KNN Classification Results:")
print(f"Accuracy: {knn_accuracy}")
print(f"Precision: {knn_precision}")
print(f"Recall: {knn_recall}")
print(f"F1-Score: {knn_f1}")

# Store results in a dictionary for later comparison
knn_results = {
    'Model': 'KNN',
    'Accuracy': knn_accuracy,
    'Precision': knn_precision,
    'Recall': knn_recall,
    'F1-Score': knn_f1
}

"""### **Model 7: Voting Classifier (LogReg + SVM + RandomForest)**"""

# Define base classifiers (you can include the classifiers you want to vote on)
rf = RandomForestClassifier()
lr = LogisticRegression()
svm = SVC()

# Create the Voting Classifier (Voting='hard' for majority voting, 'soft' for probability averaging)
voting_clf = VotingClassifier(estimators=[('rf', rf), ('lr', lr), ('svm', svm)], voting='hard')

# Train the Voting Classifier and make predictions
voting_clf.fit(X_train, y_train)
y_pred_voting = voting_clf.predict(X_test)

# Calculate metrics with zero_division handling
voting_accuracy = round(accuracy_score(y_test, y_pred_voting), 3)
voting_precision = round(precision_score(y_test, y_pred_voting, average='weighted', zero_division=1), 3)
voting_recall = round(recall_score(y_test, y_pred_voting, average='weighted'), 3)
voting_f1 = round(f1_score(y_test, y_pred_voting, average='weighted'), 3)

# Print classification results
print("Voting Classifier Classification Results:")
print(f"Accuracy: {voting_accuracy}")
print(f"Precision: {voting_precision}")
print(f"Recall: {voting_recall}")
print(f"F1-Score: {voting_f1}")

# Store results in a dictionary for later comparison
voting_results = {
    'Model': 'Voting Classifier',
    'Accuracy': voting_accuracy,
    'Precision': voting_precision,
    'Recall': voting_recall,
    'F1-Score': voting_f1
}

"""### **Model 8: Stacking Classifier (LogReg + RandomForest + SVM)**"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define base models (classifiers) for stacking
base_learners = [
    ('rf', RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_leaf=2, min_samples_split=2)),
    ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3))
]

# Define meta-model
meta_model = LogisticRegression(max_iter=1000)

# Define the Stacking Classifier
stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_model)

# Train the Stacking Classifier
stacking_clf.fit(X_train, y_train)

# Make predictions
y_pred = stacking_clf.predict(X_test)

# Calculate metrics
stacking_accuracy = round(accuracy_score(y_test, y_pred), 3)
stacking_precision = round(precision_score(y_test, y_pred, average='weighted'), 3)
stacking_recall = round(recall_score(y_test, y_pred, average='weighted'), 3)
stacking_f1 = round(f1_score(y_test, y_pred, average='weighted'), 3)

# Printing classification results
print("Stacking Classifier Results:")
print(f"Accuracy: {stacking_accuracy}")
print(f"Precision: {stacking_precision}")
print(f"Recall: {stacking_recall}")
print(f"F1-Score: {stacking_f1}")

# Storing results in a dict
stacking_results = {
    'Model': 'Stacking Classifier',
    'Accuracy': stacking_accuracy,
    'Precision': stacking_precision,
    'Recall': stacking_recall,
    'F1-Score': stacking_f1,
    'Base Learners': base_learners,
    'Meta Model': 'Logistic Regression'
}

# Optionally, print the base learners and meta model
print(f"Base Learners: {stacking_results['Base Learners']}")
print(f"Meta Model: {stacking_results['Meta Model']}")

"""# **Comparative analysis**"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Store all performance metrics in dictionaries for comparison
metrics = {
    'Logistic Regression': {'Accuracy': lr_accuracy, 'Precision': lr_precision, 'Recall': lr_recall, 'F1-Score': lr_f1},
    'SVM': {'Accuracy': svm_accuracy, 'Precision': svm_precision, 'Recall': svm_recall, 'F1-Score': svm_f1},
    'KNN': {'Accuracy': knn_accuracy, 'Precision': knn_precision, 'Recall': knn_recall, 'F1-Score': knn_f1},
    'Random Forest': {'Accuracy': rf_accuracy, 'Precision': rf_precision, 'Recall': rf_recall, 'F1-Score': rf_f1},
    'Gaussian Naive Bayes': {'Accuracy': gnb_accuracy, 'Precision': gnb_precision, 'Recall': gnb_recall, 'F1-Score': gnb_f1},
    'AdaBoost': {'Accuracy': ada_accuracy, 'Precision': ada_precision, 'Recall': ada_recall, 'F1-Score': ada_f1},
    'Voting Classifier': {'Accuracy': voting_accuracy, 'Precision': voting_precision, 'Recall': voting_recall, 'F1-Score': voting_f1},
    'Stacking Classifier': {'Accuracy': stacking_accuracy, 'Precision': stacking_precision, 'Recall': stacking_recall, 'F1-Score': stacking_f1},
    'XGBoost': {'Accuracy': xgb_accuracy, 'Precision': xgb_precision, 'Recall': xgb_recall, 'F1-Score': xgb_f1}
}

# Prepare the data for plotting
metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
model_names = list(metrics.keys())

# Set up the plot for all metrics
fig, axes = plt.subplots(1, 4, figsize=(20, 6))

# Plot each metric
for i, metric in enumerate(metric_names):
    # Extract the values for the current metric
    model_metric_values = [metrics[model][metric] for model in model_names]

    # Sort models by the current metric
    sorted_metrics = sorted(zip(model_metric_values, model_names))
    sorted_metric_values, sorted_model_names = zip(*sorted_metrics)

    # Create the bar plot
    axes[i].barh(sorted_model_names, sorted_metric_values, color='#3274a1')
    axes[i].set_xlabel(metric, fontsize=12)
    axes[i].set_title(f'{metric} Comparison', fontsize=14)

    # Add the metric values on the bars
    for j, v in enumerate(sorted_metric_values):
        axes[i].text(v + 0.01, j, f'{v:.3f}', ha='left', va='center', fontweight='bold')

# Add overall title and adjust layout
fig.suptitle('Model Performance Comparison', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to prevent overlap with suptitle

# Show the plots
plt.show()
